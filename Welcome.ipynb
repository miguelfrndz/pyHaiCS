{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to pyHaiCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyHaiCS as haics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pyHaiCS v.0.0.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running pyHaiCS v.{haics.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Bayesian Logistic Regression + HMC for Breast Cancer Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data & convert to jax arrays\n",
    "scaler = StandardScaler()\n",
    "X_train = jnp.array(scaler.fit_transform(X_train))\n",
    "X_test = jnp.array(scaler.transform(X_test))\n",
    "\n",
    "# Add column of ones to the input data (for intercept terms)\n",
    "X_train = jnp.hstack([X_train, jnp.ones((X_train.shape[0], 1))])\n",
    "X_test = jnp.hstack([X_test, jnp.ones((X_test.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Logistic Regression model (in JAX)\n",
    "@jax.jit\n",
    "def model_fn(params, x):\n",
    "    return jax.nn.sigmoid(jnp.matmul(x, params))\n",
    "\n",
    "@jax.jit\n",
    "def log_prior_fn(params):\n",
    "    return jnp.sum(jax.scipy.stats.norm.logpdf(params))\n",
    "\n",
    "@jax.jit\n",
    "def likelihood_fn(params, x, y):\n",
    "    preds = model_fn(params, x)\n",
    "    return jnp.prod(preds ** y * (1 - preds) ** (1 - y))\n",
    "\n",
    "@jax.jit\n",
    "def log_likelihood_fn(params, x, y):\n",
    "    preds = model_fn(params, x)\n",
    "    return jnp.sum(y * jnp.log(preds) + (1 - y) * jnp.log(1 - preds))\n",
    "\n",
    "@jax.jit\n",
    "def log_posterior_fn(params, x, y):\n",
    "    return log_prior_fn(params) + log_likelihood_fn(params, x, y)\n",
    "\n",
    "# Define a wrapper function to negate the log posterior\n",
    "@jax.jit\n",
    "def neg_log_posterior_fn(params, x, y):\n",
    "    return -log_posterior_fn(params, x, y)\n",
    "\n",
    "# Initialize the model parameters (includes intercept term)\n",
    "key = jax.random.PRNGKey(42)\n",
    "mean_vector = jnp.zeros(X_train.shape[1])\n",
    "cov_mat = jnp.eye(X_train.shape[1])\n",
    "params = jax.random.multivariate_normal(key, mean_vector, cov_mat)\n",
    "\n",
    "# HMC for posterior sampling\n",
    "# params_samples = haics.samplers.hamiltonian.HMC(params, n_samples=5000, burn_in=2000, \n",
    "#                             step_size=1e-3, n_steps=100, \n",
    "#                             potential=neg_log_posterior_fn,  \n",
    "#                             mass_matrix=jnp.eye(X_train.shape[1]), \n",
    "#                             integrator=haics.integrators.VerletIntegrator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [03:55<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1315789520740509\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Learn the model parameters using HMC\n",
    "num_samples = 1000\n",
    "num_burnin = 200\n",
    "step_size = 1e-3\n",
    "num_leapfrog_steps = 100\n",
    "\n",
    "@jax.jit\n",
    "def Hamiltonian(params, x, y):\n",
    "    return -log_posterior_fn(params, x, y)\n",
    "\n",
    "\n",
    "def hmc_step(params, x, y, step_size, num_leapfrog_steps, key, grad_hamiltonian):\n",
    "    @jax.jit\n",
    "    def leapfrog(params, momentum, step_size):\n",
    "        grads = grad_hamiltonian(params, x, y)\n",
    "        momentum = momentum - 0.5 * step_size * grads\n",
    "        params = params + step_size * momentum\n",
    "        grads = grad_hamiltonian(params, x, y)\n",
    "        momentum = momentum - 0.5 * step_size * grads\n",
    "        return params, momentum\n",
    "\n",
    "    momentum = jax.random.normal(key, shape=params.shape)\n",
    "    new_params, new_momentum = params, momentum\n",
    "\n",
    "    for _ in range(num_leapfrog_steps):\n",
    "        new_params, new_momentum = leapfrog(new_params, new_momentum, step_size)\n",
    "\n",
    "    current_H = Hamiltonian(params, x, y) + 0.5 * jnp.sum(momentum ** 2)\n",
    "    new_H = Hamiltonian(new_params, x, y) + 0.5 * jnp.sum(new_momentum ** 2)\n",
    "\n",
    "    accept_prob = jnp.exp(current_H - new_H)\n",
    "    accept = jax.random.uniform(key) < accept_prob\n",
    "\n",
    "    return jax.lax.cond(accept, lambda _: new_params, lambda _: params, operand=None)\n",
    "\n",
    "# Run HMC\n",
    "samples = []\n",
    "grad_hamiltonian = jax.grad(Hamiltonian)\n",
    "\n",
    "for i in tqdm(range(num_samples + num_burnin)):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    params = hmc_step(params, X_train, y_train, step_size, num_leapfrog_steps, subkey, grad_hamiltonian)\n",
    "    if i >= num_burnin:\n",
    "        samples.append(params)\n",
    "\n",
    "samples = jnp.array(samples)\n",
    "\n",
    "# Make predictions using the posterior samples\n",
    "preds = jax.vmap(lambda params: model_fn(params, X_test))(samples)\n",
    "mean_preds = jnp.mean(preds, axis=0)\n",
    "mean_preds = mean_preds > 0.5\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = jnp.mean(mean_preds == y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/pymc/data.py:317: FutureWarning: MutableData is deprecated. All Data variables are now mutable. Use Data instead.\n",
      "  warnings.warn(\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "HamiltonianMC: [beta]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c32f2b52bae4f5292dc6d73c3ea6409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 1 chain for 200 tune and 1_000 draw iterations (200 + 1_000 draws total) took 4 seconds.\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sampling: [beta, y_obs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7db392822a4009bf33b05d8cc4c323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7456140350877193\n"
     ]
    }
   ],
   "source": [
    "# Do the same using pymc (same model, and parameters) Use numpy instead of Jax\n",
    "import pymc as pm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    X_pymc = pm.MutableData('x', X_train)\n",
    "    y_pymc = pm.MutableData('y', y_train)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=1, shape=X_pymc.shape[1])\n",
    "\n",
    "    preds = pm.math.sigmoid(pm.math.dot(X_pymc, beta))\n",
    "    y_obs = pm.Bernoulli(\"y_obs\", p=preds, observed=y_pymc)\n",
    "\n",
    "    trace = pm.sample(num_samples, tune=num_burnin, step=pm.HamiltonianMC(adapt_step_size = False), chains = 1)\n",
    "\n",
    "with model:\n",
    "    pm.set_data({'x': X_test, 'y': y_test})\n",
    "    # Make predictions using the posterior samples\n",
    "    trace.extend(pm.sample_posterior_predictive(trace))\n",
    "    p_test_pred = trace.posterior_predictive[\"y_obs\"].mean(dim=[\"chain\", \"draw\"])\n",
    "    y_test_pred = (p_test_pred >= 0.5).astype(\"int\").to_numpy()\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
