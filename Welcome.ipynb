{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to pyHaiCS\n",
    "\n",
    "In this introduction notebook we provide a simple implementation of a Bayesian Logistic Regression (BLR) model so that users can become familiarized with our library and how to implement their own computational statistics models.\n",
    "\n",
    "First, we begin by importing `pyHaiCS`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyHaiCS as haics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pyHaiCS v.0.0.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running pyHaiCS v.{haics.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Bayesian Logistic Regression + HMC for Breast Cancer Classification\n",
    "\n",
    "As a *toy example*, we implement below a *classic* BLR classifier for predicting (binary) breast cancer outcomes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data & convert to jax arrays\n",
    "scaler = StandardScaler()\n",
    "X_train = jnp.array(scaler.fit_transform(X_train))\n",
    "X_test = jnp.array(scaler.transform(X_test))\n",
    "\n",
    "# Add column of ones to the input data (for intercept terms)\n",
    "X_train = jnp.hstack([X_train, jnp.ones((X_train.shape[0], 1))])\n",
    "X_test = jnp.hstack([X_test, jnp.ones((X_test.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we train a baseline *point-estimate* logistic regression model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (w/ Scikit-Learn Baseline): 0.974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def baseline_classifier(X_train, y_train, X_test, y_test):\n",
    "    clf = LogisticRegression(random_state = 42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy (w/ Scikit-Learn Baseline): {accuracy:.3f}\\n\")\n",
    "\n",
    "baseline_classifier(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to implement its Bayesian counterpart, we start by writing the model and the Hamiltonian potential (i.e., the negative log-posterior)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Logistic Regression model (in JAX)\n",
    "@jax.jit\n",
    "def model_fn(x, params):\n",
    "    return jax.nn.sigmoid(jnp.matmul(x, params))\n",
    "\n",
    "@jax.jit\n",
    "def prior_fn(params):\n",
    "    return jax.scipy.stats.norm.pdf(params)\n",
    "\n",
    "@jax.jit\n",
    "def log_prior_fn(params):\n",
    "    return jnp.sum(jax.scipy.stats.norm.logpdf(params))\n",
    "\n",
    "@jax.jit\n",
    "def likelihood_fn(x, y, params):\n",
    "    preds = model_fn(x, params)\n",
    "    return jnp.prod(preds ** y * (1 - preds) ** (1 - y))\n",
    "\n",
    "@jax.jit\n",
    "def log_likelihood_fn(x, y, params):\n",
    "    epsilon = 1e-7\n",
    "    preds = model_fn(x, params)\n",
    "    return jnp.sum(y * jnp.log(preds + epsilon) + (1 - y) * jnp.log(1 - preds + epsilon))\n",
    "\n",
    "@jax.jit\n",
    "def posterior_fn(x, y, params):\n",
    "    return prior_fn(params) * likelihood_fn(x, y, params)\n",
    "\n",
    "@jax.jit\n",
    "def log_posterior_fn(x, y, params):\n",
    "    return log_prior_fn(params) + log_likelihood_fn(x, y, params)\n",
    "\n",
    "@jax.jit\n",
    "def neg_posterior_fn(x, y, params):\n",
    "    return -posterior_fn(x, y, params)\n",
    "\n",
    "# Define a wrapper function to negate the log posterior\n",
    "@jax.jit\n",
    "def neg_log_posterior_fn(x, y, params):\n",
    "    return -log_posterior_fn(x, y, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can call the `HMC` sampler in `pyHaiCS` (and sampler from several chains at once) with a very *high-level* interface..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running HMC sampler...\n",
      "=============================================================\n",
      "         Num. Chains          |              4               \n",
      "         Num. Samples         |             1000             \n",
      "   Num. Burn-In Iterations    |             200              \n",
      "          Step-Size           |            0.001             \n",
      "    Num. Integration Steps    |             100              \n",
      "=============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [01:37<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (w/ HMC Sampling): 0.9736841917037964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model parameters (includes intercept term)\n",
    "key = jax.random.PRNGKey(42)\n",
    "mean_vector = jnp.zeros(X_train.shape[1])\n",
    "cov_mat = jnp.eye(X_train.shape[1])\n",
    "params = jax.random.multivariate_normal(key, mean_vector, cov_mat)\n",
    "\n",
    "# HMC for posterior sampling\n",
    "params_samples = haics.samplers.hamiltonian.HMC(params, \n",
    "                            potential_args = (X_train, y_train),                                           \n",
    "                            n_samples = 1000, burn_in = 200, \n",
    "                            step_size = 1e-3, n_steps = 100, \n",
    "                            potential = neg_log_posterior_fn,  \n",
    "                            mass_matrix = jnp.eye(X_train.shape[1]), \n",
    "                            integrator = haics.integrators.VerletIntegrator(), \n",
    "                            RNG_key = 120)\n",
    "\n",
    "# Average across chains\n",
    "params_samples = jnp.mean(params_samples, axis = 0)\n",
    "\n",
    "# Make predictions using the samples\n",
    "preds = jax.vmap(lambda params: model_fn(X_test, params))(params_samples)\n",
    "mean_preds = jnp.mean(preds, axis = 0)\n",
    "mean_preds = mean_preds > 0.5\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = jnp.mean(mean_preds == y_test)\n",
    "print(f\"Accuracy (w/ HMC Sampling): {accuracy}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
